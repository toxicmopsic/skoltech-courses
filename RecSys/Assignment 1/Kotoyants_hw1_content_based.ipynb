{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "898f285b",
      "metadata": {
        "id": "898f285b"
      },
      "source": [
        "# Content-based models (20 PTS)\n",
        "\n",
        "In this set, you need to implement a content-based approach to solve the ranking problem. Moreover, you should add some personalization for model as a result, to provide a list of personal recommendations for every user. Thus we need to exploit the information on user-item interactions. It could be done in two ways:\n",
        "\n",
        "**1)** Constructing content-based models for every user in the dataset\n",
        "\n",
        "**2)** Constructing user profiles\n",
        "\n",
        "To evaluate your solution, you need a new metric. As this is a ranking problem, we will use $Recall@n$. $Recall@n$ will be calculated for each user individualy. \n",
        "\n",
        "$$\n",
        "Recall_u @n = \\frac{|anime_u \\cap holdout_u|}{|holdout_u|}\n",
        "$$\n",
        "\n",
        "Holdout items here are the items our model will not see during the training. \n",
        "Each user has his/her own holdout items.\n",
        "You will need a holdout in the evaluation step.\n",
        "In this step, we predict the top $N$ recommended animes. We expect that the holdout items will be within recommended items."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d01225",
      "metadata": {
        "id": "c3d01225"
      },
      "source": [
        "\n",
        "\n",
        "## Content-based models with personalization (10 PTS)\n",
        "\n",
        "In this problem you need to implement simple content-based model for each user individually in order to achieve some level of personalization. Thus your model may be considered as ensemble of the personal models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380b92a9",
      "metadata": {
        "id": "380b92a9"
      },
      "source": [
        "Here we present some default functions which are used in the code below. Do not change them. Note, that the functions below are improved versions of the functions from the seminar."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# polara\n",
        "!pip install --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara\n",
        "\n",
        "# ipypb:\n",
        "!pip install ipypb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWjGm4zjYyKu",
        "outputId": "8adc55a5-210c-4a19-eb6f-b7648b003326"
      },
      "id": "HWjGm4zjYyKu",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting polara\n",
            "  Cloning https://github.com/evfro/polara.git (to revision develop) to /tmp/pip-install-mrnsr3__/polara_f13e4048db4348098c5cf5802f4b2697\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/evfro/polara.git /tmp/pip-install-mrnsr3__/polara_f13e4048db4348098c5cf5802f4b2697\n",
            "  Running command git checkout -b develop --track origin/develop\n",
            "  Switched to a new branch 'develop'\n",
            "  Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
            "  Resolved https://github.com/evfro/polara.git to commit 8e48cfd88e616ca53f8bbda1702a3e2c8abaf38e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: polara\n",
            "  Building wheel for polara (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polara: filename=polara-0.7.2.dev0-py3-none-any.whl size=89470 sha256=0ec769456a578c29462559cc9a277b9e21566ad80e1941ebeaf868203b79cd2a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e_8xp9n_/wheels/d2/cf/bf/e1a3e49c4a733261d717fa4732eadf0303dabcc48ba694ad7a\n",
            "Successfully built polara\n",
            "Installing collected packages: polara\n",
            "Successfully installed polara-0.7.2.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipypb\n",
            "  Downloading ipypb-0.5.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: ipypb\n",
            "Successfully installed ipypb-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z00QI_OzT_Oh",
        "outputId": "bb039e47-f847-4c66-ac15-314c904d7b3a"
      },
      "id": "z00QI_OzT_Oh",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Skoltech/RecSys/Assignment 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6TmFmigUNfY",
        "outputId": "be8d564b-6200-4a6b-b2cc-e91bf17b9397"
      },
      "id": "Y6TmFmigUNfY",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Skoltech/RecSys/Assignment 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0124170b",
      "metadata": {
        "id": "0124170b"
      },
      "outputs": [],
      "source": [
        "from ast import literal_eval\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from evaluation import topidx\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "710e80e8",
      "metadata": {
        "id": "710e80e8"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "data_r = zipfile.ZipFile('./reviews_cut.csv.zip', 'r')\n",
        "data_r.printdir()\n",
        "\n",
        "# extract the contents\n",
        "data_r.extractall()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAVF8nQLU9WK",
        "outputId": "3a8f1103-b4a5-43cf-ae9a-b42bede2eb37"
      },
      "id": "dAVF8nQLU9WK",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Name                                             Modified             Size\n",
            "reviews_cut.csv                                2023-02-03 09:22:38    697207543\n",
            "__MACOSX/._reviews_cut.csv                     2023-02-03 09:22:38          212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a740197d",
      "metadata": {
        "id": "a740197d"
      },
      "outputs": [],
      "source": [
        "anime_cut = (\n",
        "    pd.read_csv('anime_cut.csv')\n",
        "    .dropna() # remove items w/o description\n",
        "    # remove items with empty string as descriptions\n",
        "    .loc[lambda x: x['synopsis'].str.strip().apply(len)>0]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "194dd70f",
      "metadata": {
        "id": "194dd70f"
      },
      "outputs": [],
      "source": [
        "def string_ids_to_ints(line, allowed_ids):\n",
        "    '''\n",
        "    Convert text representation of ids list into python list of integers.\n",
        "    Filter out ids that are not present in allowed ids.\n",
        "    '''\n",
        "    return [int(x) for x in literal_eval(line) if int(x) in allowed_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b8c7902e",
      "metadata": {
        "id": "b8c7902e"
      },
      "outputs": [],
      "source": [
        "\n",
        "allowed_items = set(anime_cut['anime_id'].values)\n",
        "\n",
        "reviews_cut = (\n",
        "    pd.read_csv('reviews_cut.csv')\n",
        "    .drop_duplicates(subset=['user_id', 'anime_id'])\n",
        "    .query('anime_id in @allowed_items') # ensure review texts are present\n",
        "    .assign(# convert favorites data into lists of integer ids \n",
        "        favorites_anime = lambda x:\n",
        "            x['favorites_anime']\n",
        "            .apply(string_ids_to_ints, args=(allowed_items,))\n",
        "    )\n",
        "    .loc[lambda x: x['favorites_anime'].apply(len)>0] # drop users without favorites\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c1f252b",
      "metadata": {
        "id": "1c1f252b"
      },
      "source": [
        "## Getting test triplets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "131f4fab",
      "metadata": {
        "id": "131f4fab"
      },
      "outputs": [],
      "source": [
        "def split_train_test(reviews, n_pairs, score_cutoff=5, seed=0):\n",
        "    \"\"\"\n",
        "    Splits anime rating data into training and test sets for content-based filtering.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    reviews : pandas.DataFrame\n",
        "        DataFrame containing ratings data.\n",
        "    n_pairs : int\n",
        "        The number of liked, disliked anime items to select per test user.\n",
        "    score_cutoff : int, optional\n",
        "        The cutoff threshold for item ratings. Items with ratings below this threshold are considered disliked. Default is 5.\n",
        "    seed : int, optional\n",
        "        Random seed to ensure reproducibility. Default is 0.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple of pandas.DataFrame\n",
        "        A tuple containing the training and test datasets.\n",
        "        - The training DataFrame contains the user ID, anime ID, and rating for each item.\n",
        "        - The test DataFrame contains triplets of liked, disliked, and favorite anime items for a subset of test users.\n",
        "\n",
        "    TL;DR\n",
        "    -----\n",
        "    Collects triplets of liked, disliked and favorite items for a subset of test users.\n",
        "    The remaining items of the selected test users are used for training CB models.\n",
        "    \"\"\"    \n",
        "    # select only users with at least 1 anime in favorites\n",
        "    subset = reviews.loc[\n",
        "        reviews[\"favorites_anime\"].apply(lambda x : len(x) >0),\n",
        "        ['user_id', 'anime_id', 'score', 'favorites_anime']\n",
        "    ]\n",
        "    valid_users = users_with_enough_data(subset, score_cutoff, n_pairs)\n",
        "    # select only valid users (i.e. with enough likes and dislikes) and shuffle\n",
        "    user_selection = subset.query('user_id in @valid_users')\n",
        "    likes_dislikes = gather_user_feedback(user_selection, score_cutoff, n_pairs, seed)\n",
        "    # extract favorites data\n",
        "    favorites = (\n",
        "        user_selection\n",
        "        .drop_duplicates(subset=['user_id'])\n",
        "        .set_index('user_id')\n",
        "        ['favorites_anime']\n",
        "    )\n",
        "    # combine likes, dislikes, and triplets into single dataframe\n",
        "    test_triplets = pd.merge(\n",
        "        likes_dislikes,\n",
        "        favorites,\n",
        "        left_index=True,\n",
        "        right_index=True,\n",
        "        how='inner'\n",
        "    )\n",
        "    # for each user, exclude test items from training\n",
        "    test_data = (\n",
        "        test_triplets\n",
        "        #.eval('likes + dislikes + favorites_anime')\n",
        "        .sum(axis=1)\n",
        "        .explode()\n",
        "        .to_frame('anime_id')\n",
        "        .reset_index()\n",
        "    )\n",
        "    all_data = user_selection[['user_id', 'anime_id', 'score']]\n",
        "    train_data = pd.merge(\n",
        "        all_data,\n",
        "        test_data,\n",
        "        on=['user_id', 'anime_id'],\n",
        "        indicator=True, # test entries will be indicated as \"both\" or \"right_only\"\n",
        "        how='left', # train entries will be indicated as \"left_only\"\n",
        "    ).query('_merge == \"left_only\"') # select train entries only\n",
        "    return train_data.drop('_merge', axis=1), test_triplets.sort_index()\n",
        "\n",
        "\n",
        "def users_with_enough_data(data, score_cutoff, n_pairs):\n",
        "    '''\n",
        "    Return users that have enough positive and negative items.\n",
        "    '''\n",
        "    valid_users = (\n",
        "        (data[\"score\"] >= score_cutoff)\n",
        "        .groupby(data['user_id'])\n",
        "        .agg(total='size', n_positive='sum')\n",
        "        .assign(n_negative=lambda x: x.eval('total - n_positive'))\n",
        "        .eval('n_positive >= @n_pairs and n_negative >= @n_pairs')\n",
        "    )\n",
        "    return valid_users.loc[lambda x: x].index\n",
        "\n",
        "\n",
        "def gather_user_feedback(data, score_cutoff, n_pairs, seed):\n",
        "    '''\n",
        "    Extract fixed number of likes and dislikes per each user.\n",
        "    '''\n",
        "    likes_dislikes = (\n",
        "        data\n",
        "        # shuffle data to randomize selection of items\n",
        "        .sample(frac=1, random_state=seed)\n",
        "        # group items by positive/negative score for each user\n",
        "        .groupby(['user_id', data['score']>=score_cutoff])\n",
        "        ['anime_id'].apply(list) # make pos/neg item lists\n",
        "        .str[:n_pairs] # select fixed number of pos/neg items per user\n",
        "        .unstack('score') # pos/neg class as columns in dataframe\n",
        "    )\n",
        "    return likes_dislikes.rename(columns={False: 'dislikes', True: 'likes'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a06f620a",
      "metadata": {
        "id": "a06f620a"
      },
      "source": [
        "## Prepearing the data\n",
        "\n",
        "Here we prepare the data. We divide  the original dataset into two disjoint parts so that for every user his\\her train history does not include likes and dislikes from test triplets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6f3f61f8",
      "metadata": {
        "id": "6f3f61f8"
      },
      "outputs": [],
      "source": [
        "reviews_train, test_triplets_ = split_train_test(reviews_cut, 3, score_cutoff=5, seed=0)\n",
        "test_triplets = test_triplets_[test_triplets_.index.isin(reviews_train['user_id'].unique())]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MWGhoLO0WrzJ",
        "outputId": "89139880-4089-401e-9403-ae3d48ddfc21"
      },
      "id": "MWGhoLO0WrzJ",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   user_id anime_id  score\n",
              "0        6    16664      6\n",
              "1       10     4672      8\n",
              "2       13    34599      8\n",
              "3       31     1425      7\n",
              "4       33     1425      6"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96bf0535-3842-41cc-9496-6ab6aa60317e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>anime_id</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>16664</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>4672</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13</td>\n",
              "      <td>34599</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31</td>\n",
              "      <td>1425</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>1425</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96bf0535-3842-41cc-9496-6ab6aa60317e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-96bf0535-3842-41cc-9496-6ab6aa60317e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-96bf0535-3842-41cc-9496-6ab6aa60317e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_triplets.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "L1Dvj77KmHDu",
        "outputId": "72a85403-9e14-406d-d741-839d0af12c43"
      },
      "id": "L1Dvj77KmHDu",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      dislikes                  likes  \\\n",
              "user_id                                                 \n",
              "6        [32901, 32281, 11757]     [338, 1441, 33731]   \n",
              "10            [1221, 413, 841]         [195, 59, 578]   \n",
              "13       [35073, 33926, 35790]  [35557, 30736, 31433]   \n",
              "31            [537, 8861, 384]      [12031, 166, 129]   \n",
              "33              [95, 92, 2770]      [732, 1795, 2402]   \n",
              "\n",
              "                                           favorites_anime  \n",
              "user_id                                                     \n",
              "6        [338, 322, 440, 199, 28223, 12815, 2800, 18679...  \n",
              "10                         [534, 71, 7724, 861, 5060, 853]  \n",
              "13       [27989, 28851, 17265, 15227, 35557, 24, 5420, ...  \n",
              "31                              [512, 7193, 165, 440, 634]  \n",
              "33                       [21, 235, 19, 302, 90, 474, 1453]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca7d7d9d-eead-4b9a-b32b-62439b764b54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dislikes</th>\n",
              "      <th>likes</th>\n",
              "      <th>favorites_anime</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>user_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[32901, 32281, 11757]</td>\n",
              "      <td>[338, 1441, 33731]</td>\n",
              "      <td>[338, 322, 440, 199, 28223, 12815, 2800, 18679...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[1221, 413, 841]</td>\n",
              "      <td>[195, 59, 578]</td>\n",
              "      <td>[534, 71, 7724, 861, 5060, 853]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[35073, 33926, 35790]</td>\n",
              "      <td>[35557, 30736, 31433]</td>\n",
              "      <td>[27989, 28851, 17265, 15227, 35557, 24, 5420, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[537, 8861, 384]</td>\n",
              "      <td>[12031, 166, 129]</td>\n",
              "      <td>[512, 7193, 165, 440, 634]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[95, 92, 2770]</td>\n",
              "      <td>[732, 1795, 2402]</td>\n",
              "      <td>[21, 235, 19, 302, 90, 474, 1453]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca7d7d9d-eead-4b9a-b32b-62439b764b54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca7d7d9d-eead-4b9a-b32b-62439b764b54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca7d7d9d-eead-4b9a-b32b-62439b764b54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a95799a",
      "metadata": {
        "id": "6a95799a"
      },
      "source": [
        "## Creating the model\n",
        "\n",
        "Let's come down to business!\n",
        "\n",
        "- Build a collection of regression-based CB models on the anime data from Lecture 2.\n",
        "\n",
        "- Pay attention that now you are asked to build $N$\n",
        "CB models for every user separately, taking only users' history into account. \n",
        "\n",
        "But if your model considers only synopsises of the animes from user history, your model may not see all the words from all the synopsises. Hence, during the evaluation, some features (words) will be omitted, which affects the model's predictions. Suggest the solution of this problem.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dbb942ff",
      "metadata": {
        "id": "dbb942ff"
      },
      "outputs": [],
      "source": [
        "cb_config = {\n",
        "    \"tfidf\": dict( # TfIDF Vectorizer config\n",
        "        ngram_range = (1, 1),\n",
        "        min_df=1, max_df=4,\n",
        "        strip_accents='unicode',\n",
        "        stop_words = 'english',\n",
        "        analyzer = 'word',\n",
        "        use_idf=True,\n",
        "        smooth_idf=True,\n",
        "        sublinear_tf=True\n",
        "    ),\n",
        "}\n",
        "# we also define a general representation of our dataset\n",
        "anime_description = {\n",
        "    'users': 'user_id',\n",
        "    'items': 'anime_id',\n",
        "    'favorites': 'favorites_anime',\n",
        "    'feedback' : 'score',\n",
        "    'feature_map': anime_cut.set_index('anime_id')['synopsis'],\n",
        "    'train_items': reviews_train['anime_id'].unique()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c9e7cd13",
      "metadata": {
        "id": "c9e7cd13"
      },
      "outputs": [],
      "source": [
        "def build_cb_model(config, trainset, trainset_description):\n",
        "    \"\"\"\n",
        "    Build a set of content-based models to recommend items to users based on their history of feedback.\n",
        "    Each user has a separate model.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    config : dict\n",
        "        A dictionary containing configuration settings for the model.\n",
        "        \n",
        "    trainset : pd.DataFrame\n",
        "        A pandas DataFrame containing user-item-feedback tuples for training the model.\n",
        "        \n",
        "    trainset_description : dict\n",
        "        A dictionary containing the description of the trainset with the following keys:\n",
        "            - 'users': string\n",
        "                The name of the column containing user IDs in the trainset.\n",
        "            - 'items': string\n",
        "                The name of the column containing item IDs in the trainset.\n",
        "            - 'feedback': string\n",
        "                The name of the column containing feedback values in the trainset.\n",
        "            - 'feature_map': pd.Series\n",
        "                A Series containing item features mapped to their respective IDs.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary containing trained Linear Regression models and TfidfVectorizer objects \n",
        "        for each user ID in the trainset.\n",
        "    \"\"\"\n",
        "    userid = trainset_description['users']\n",
        "    itemid = trainset_description['items']\n",
        "    feedback = trainset_description['feedback']\n",
        "    feature_map = trainset_description['feature_map']\n",
        "    \n",
        "    train_data = trainset[[userid, itemid, feedback]].groupby(userid).agg(list)\n",
        "    users_dict = {}\n",
        "    for user_id, items, ratings in train_data.itertuples(name=None):\n",
        "        # we iterate over rows of `train_data` Dataframe\n",
        "        # note that by construction, `train_data`'s index encodes users IDs,\n",
        "        # and the two columns of `train_data` correspond to items and their ratings from user history\n",
        "        word_vectorizer = TfidfVectorizer(**config['tfidf'])\n",
        "        item_features = feature_map[items]\n",
        "        tfidf_matrix = word_vectorizer.fit_transform(item_features)\n",
        "        # print(tfidf_matrix.shape)\n",
        "        reg = LinearRegression().fit(tfidf_matrix, ratings)\n",
        "        users_dict[user_id] = (reg, word_vectorizer)\n",
        "    return users_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2777af00",
      "metadata": {
        "id": "2777af00"
      },
      "outputs": [],
      "source": [
        "cb_params = build_cb_model(cb_config, reviews_train, anime_description)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cb_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhMVK-T3A0eP",
        "outputId": "337bc9cb-b0d2-4651-cfa6-5386f9a8aaa6"
      },
      "id": "nhMVK-T3A0eP",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{6: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 13: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 31: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 33: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 42: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 48: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 55: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 58: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 76: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 77: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 79: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 82: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 95: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 97: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 104: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 110: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 112: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 114: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 117: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 118: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 122: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 126: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 133: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 146: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 159: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 194: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 198: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 228: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 257: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 263: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 271: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 280: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 282: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 285: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 286: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 287: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 298: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 300: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 312: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 324: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 327: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 332: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 333: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 337: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 344: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 355: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 360: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 376: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 394: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 397: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 410: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 422: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 440: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 447: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 449: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 451: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 455: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 456: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 457: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 460: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 461: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 463: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 470: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 473: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 477: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 480: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 483: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 512: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 516: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 518: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 519: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 521: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 523: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 524: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 525: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 527: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 531: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 550: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 568: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 572: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 574: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 584: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 588: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 592: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 593: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 596: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 597: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 609: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 610: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 621: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 632: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 641: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 648: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 650: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 651: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 657: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 660: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 670: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 689: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 693: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 694: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 696: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 724: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 726: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 732: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 742: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 751: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 753: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 765: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 771: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 789: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 807: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 809: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 834: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 843: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 855: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 857: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 868: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 890: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 901: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 902: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 907: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 919: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 926: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 929: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 934: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 935: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 936: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 940: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 948: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 958: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 959: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 962: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 964: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 966: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1011: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1020: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1022: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1052: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1057: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1077: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1078: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1103: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1110: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1116: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1121: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1130: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1148: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1201: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1207: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1226: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1228: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1298: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1308: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1309: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1335: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1341: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1347: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1361: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1370: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1374: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1382: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1399: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1400: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1417: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1424: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1426: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1445: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1457: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1459: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1462: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1482: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1483: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1484: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1486: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1508: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1513: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1518: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1524: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1528: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1530: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1542: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1545: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1546: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1550: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1551: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1552: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1555: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1558: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1564: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1574: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1585: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1588: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1599: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1619: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1620: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1623: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1629: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1630: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1634: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1643: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1650: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1653: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1656: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1660: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1671: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1673: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1676: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1686: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1694: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1699: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1704: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1762: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1784: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1803: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1827: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1832: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1833: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1855: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1882: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1888: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1893: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1929: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1941: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1943: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1944: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1948: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 1986: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2022: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2036: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2037: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2056: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2082: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2095: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2107: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2122: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2134: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2137: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2143: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2160: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2161: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2162: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2192: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2221: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2236: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2254: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2285: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2308: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2315: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2332: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2343: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2362: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2371: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2409: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2418: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2466: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2469: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2477: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2478: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2488: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2490: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2531: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2547: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2568: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2570: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2661: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2666: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2689: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2698: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2702: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2783: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2803: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2813: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2816: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2844: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2848: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2866: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2883: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2885: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2893: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2917: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2922: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2928: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2933: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2937: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2939: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2946: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2951: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2993: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 2999: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3005: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3037: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3049: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3056: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3067: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3085: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3158: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3162: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3177: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3178: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3234: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3264: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3267: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3269: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3277: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3327: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3348: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3355: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3370: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3376: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3380: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3390: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3414: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3452: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3459: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3470: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3474: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3486: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3496: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3500: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3547: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3550: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3553: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3562: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3566: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3636: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3788: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3809: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3830: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3868: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3886: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3903: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3904: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3947: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3954: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3955: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3971: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 3995: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4002: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4011: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4027: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4029: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4130: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4132: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4137: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4150: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4152: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4187: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4196: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4201: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4207: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4211: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4212: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4230: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4242: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4262: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4335: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4384: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4425: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4439: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4450: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4466: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4480: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4496: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4510: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4547: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4561: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4580: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4591: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4593: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4689: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4692: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4700: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4716: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4805: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4831: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4859: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4862: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4877: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4916: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4931: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4974: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4978: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 4999: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5013: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5033: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5055: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5076: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5154: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5164: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5229: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5234: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5258: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5261: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5267: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5361: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5410: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5438: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5495: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5517: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5545: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5566: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5589: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5609: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5615: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5624: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5625: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5636: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5652: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5688: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5748: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5775: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5777: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5788: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5806: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5810: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5897: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5912: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5948: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 5984: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6009: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6016: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6030: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6039: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6089: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6138: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6141: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6149: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6181: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6286: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6292: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6340: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6341: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6358: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6370: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6372: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6373: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6390: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6421: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6445: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6513: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6516: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6566: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6587: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6590: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6602: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6609: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6612: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6622: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6628: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6671: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6687: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6702: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6758: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6764: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6769: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6770: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6772: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6793: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6827: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6839: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6856: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6865: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 6954: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7068: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7107: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7121: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7157: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7166: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7168: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7169: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7227: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7266: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7370: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7385: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7431: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7443: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7462: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7499: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7500: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7526: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7559: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7609: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7637: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7655: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7677: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7721: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7724: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7800: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7801: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7870: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7889: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 7944: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8002: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8003: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8013: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8015: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8053: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8074: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8134: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8150: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8187: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8214: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8217: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8246: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8266: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8300: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8350: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8355: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8363: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8555: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8654: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8673: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8713: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8722: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8738: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8793: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8858: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8885: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 8908: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9123: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9314: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9368: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9417: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9447: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9451: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9475: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9588: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9604: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9642: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9683: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9778: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9842: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9935: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 9997: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10027: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10051: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10071: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10109: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10251: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10296: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10399: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10413: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10483: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10505: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10589: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10613: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10699: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10711: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10744: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10842: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10858: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 10937: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11015: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11036: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11058: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11070: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11086: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11092: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11113: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11132: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11189: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11197: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11296: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11316: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11321: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11366: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11444: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11481: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11484: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11527: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11531: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11538: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11601: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11680: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11717: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11944: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11961: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11989: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 11999: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12037: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12137: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12207: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12254: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12309: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12322: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12343: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12428: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12444: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12937: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12945: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12951: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 12988: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 13093: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 13099: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 13180: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 13411: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 13552: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 13553: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 13825: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 13940: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 14186: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 14232: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 14235: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 14319: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 15236: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 15249: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 15265: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 15440: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 15442: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 15559: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 15771: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 15969: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 16004: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 16182: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 16230: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 16321: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 16451: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 16601: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 16678: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 16890: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 16891: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 16992: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 17348: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 17603: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 17745: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 18165: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 18209: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 19090: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 19248: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 19349: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 19460: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 19501: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 20404: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 20726: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 20728: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 21500: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 21639: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 21659: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 22068: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 22180: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 22846: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 22891: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 23161: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 23247: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 23461: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 23627: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 24074: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 24083: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 24347: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 24756: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 24763: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 24896: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 24981: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 25472: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 27255: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 27564: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 27737: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 27749: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 27763: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 27799: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 27807: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 27835: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 27974: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 28015: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 28027: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 28077: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 28235: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 28276: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 28456: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 28689: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 28812: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 28952: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 29116: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 29119: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 29932: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 30080: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 30801: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 31165: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 31318: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 32333: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 33391: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 33724: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True)),\n",
              " 34104: (LinearRegression(),\n",
              "  TfidfVectorizer(max_df=4, stop_words='english', strip_accents='unicode',\n",
              "                  sublinear_tf=True))}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c52e6a60",
      "metadata": {
        "id": "c52e6a60"
      },
      "source": [
        "# Generating recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e988d9d",
      "metadata": {
        "id": "8e988d9d"
      },
      "source": [
        "In order to evaluate the model you need to pass the model's recommendations into evaluation function.\n",
        "\n",
        "- To get predictions, you need to provide the model information about animes' synopsis from triplets (likes/dislikes/favorites). Moreover, you need to provide the model the rest of the anime from the catalog $anime_{cat}$ .\n",
        "\n",
        "$$\n",
        "anime_{test} = likes + dislikes + favourites + anime_{cat}\n",
        "$$\n",
        "\n",
        "- When you pass $anime_{test}$ into your model, you will get  predictions of scores. To get $anime_u$ (list of our top $N$ recommendations) you need to sort $anime_{test}$ according to predicted score and take top $N$ items with the highest scores \n",
        "\n",
        "$$\n",
        "anime_{u} = anime_{test}[sorted scores][:N]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "a8e56722",
      "metadata": {
        "id": "a8e56722"
      },
      "outputs": [],
      "source": [
        "def cb_model_recommendations(params, training, testset, data_description, topn=10):\n",
        "    \"\"\"\n",
        "    Uses an ensemble of individual content-based models to generate recommendations for each test user.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : dict\n",
        "        A dictionary containing the regression model and word vectorizer for each user.\n",
        "    training : pandas.DataFrame\n",
        "        DataFrame containing the user ID, item ID, and rating for each item in the training dataset.\n",
        "    testset : pandas.DataFrame\n",
        "        DataFrame containing the user ID, liked items, disliked items, and favorite items for each test user.\n",
        "    data_description : dict\n",
        "        A dictionary containing metadata information for the dataset.\n",
        "        - feature_map : pandas.DataFrame\n",
        "            DataFrame containing the item ID and feature representation for each item.\n",
        "        - users : str\n",
        "            The name of the user ID column.\n",
        "        - items : str\n",
        "            The name of the anime ID column.\n",
        "        - train_items : numpy.ndarray\n",
        "            Array containing the unique item IDs from the training dataset.\n",
        "    topn : int, optional\n",
        "        The number of recommendations to generate per user. Default is 10.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "        Array containing the top n recommended anime IDs for each test user\n",
        "        with preserved ordering of rows corresponding to the order of users in testset.\n",
        "\n",
        "    TL;DR\n",
        "    -----\n",
        "    This function generates item recommendations for test users using a content-based model.\n",
        "    For each user in the testset, the function selects items not in the user's history and \n",
        "    combines them with the user's likes, dislikes, and favorites.\n",
        "    The function then applies the user's regression model to the feature representation of \n",
        "    these items and generates a score for each of them. The top-n scoring items are recommended to the user.\n",
        "\n",
        "    \"\"\"\n",
        "    feature_map = data_description['feature_map']\n",
        "    userid = data_description['users']\n",
        "    itemid = data_description['items']\n",
        "    user_history = training.groupby(userid)[itemid].apply(list)\n",
        "    all_items = data_description['train_items']\n",
        "    recs = []\n",
        "    for user_id, likes, dislikes, favs in testset.itertuples(name=None):\n",
        "        items_not_from_history = np.setdiff1d(all_items, user_history[user_id])\n",
        "        #scoring_items = likes + dislikes + favs + items_not_from_history.tolist()\n",
        "        scoring_items = np.unique(np.concatenate((items_not_from_history, likes, dislikes, favs)))\n",
        "        reg, word_vectorizer = params[user_id] \n",
        "        tfidf_matrix = word_vectorizer.transform(feature_map[scoring_items]) ## \n",
        "        user_scores = reg.predict(tfidf_matrix)\n",
        "\n",
        "        my_dict = dict(zip(scoring_items, user_scores))\n",
        "        sorted_items = sorted(my_dict.items(), key=lambda x: x[-1], reverse=True)\n",
        "        top_n_items = dict(sorted_items[:topn])\n",
        "\n",
        "        # #user_recs = topidx(user_scores, topn)\n",
        "\n",
        "        user_recs = list(top_n_items.keys())\n",
        "        # ##user_recs = sorted(user_scores, reverse=True)[:topn]\n",
        "\n",
        "        #user_recs = np.array(sorted(zip(scoring_items, user_scores), key=lambda x: x[-1],\n",
        "        #                            reverse=True)[:topn])[:,0]\n",
        "\n",
        "\n",
        "        recs.append(user_recs)\n",
        "\n",
        "    return np.array(recs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "8bc56db8",
      "metadata": {
        "id": "8bc56db8"
      },
      "outputs": [],
      "source": [
        "cb_recs = cb_model_recommendations(cb_params, reviews_train, test_triplets, anime_description)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cb_recs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW3vTCW5FUE8",
        "outputId": "3e8ce626-699a-42b9-e880-010bd0658bb0"
      },
      "id": "pW3vTCW5FUE8",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14349, 11553,    24, ...,   687, 11979, 11977],\n",
              "       [11979,   457,  5569, ...,  8464, 17635, 21939],\n",
              "       [ 1572,  2154, 10161, ..., 13405,   475,   799],\n",
              "       ...,\n",
              "       [   79, 37281, 32491, ..., 34973, 37033,   929],\n",
              "       [ 2001, 33051, 34451, ..., 34176, 36816, 31338],\n",
              "       [    1,     5,     6, ...,    20,    21,    22]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cb_recs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yggaDGCADEAh",
        "outputId": "de87a82d-b2ca-4ecd-d930-701cb99de119"
      },
      "id": "yggaDGCADEAh",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2533, 2361,   27, ...,  453, 2399, 2398],\n",
              "       [2182,  264, 1606, ...,   32, 1863, 2497],\n",
              "       [ 902, 1123, 2222, ..., 2497,  338,  503],\n",
              "       ...,\n",
              "       [  69, 4092, 3511, ..., 3819, 4066,  576],\n",
              "       [1067, 3579, 3742, ..., 3704, 4028, 3364],\n",
              "       [1460, 1465, 1458, ..., 1453, 1452, 4365]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cb_recs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lZeXDZoZndS",
        "outputId": "7d6afa41-7c6f-4acd-ce3f-e5f1fc23f3a1"
      },
      "id": "2lZeXDZoZndS",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14349, 11553,    24, ...,   687, 11979, 11977],\n",
              "       [11979,   457,  5569, ...,  8464, 17635, 21939],\n",
              "       [ 1572,  2154, 10161, ..., 13405,   475,   799],\n",
              "       ...,\n",
              "       [   79, 37281, 32491, ..., 34973, 37033,   929],\n",
              "       [ 2001, 33051, 34451, ..., 34176, 36816, 31338],\n",
              "       [ 9331,   270,  9330, ...,     1,   934,    72]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cb_recs.shape"
      ],
      "metadata": {
        "id": "yfyDDeuz17xy"
      },
      "id": "yfyDDeuz17xy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reviews_train"
      ],
      "metadata": {
        "id": "Ug629-eViF0o"
      },
      "id": "Ug629-eViF0o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_triplets"
      ],
      "metadata": {
        "id": "7gL0aF06iHeD"
      },
      "id": "7gL0aF06iHeD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bfaa5dd8",
      "metadata": {
        "id": "bfaa5dd8"
      },
      "source": [
        "# EVALUATION AND HOLDOUT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecdc498b",
      "metadata": {
        "id": "ecdc498b"
      },
      "source": [
        "## HOLDOUT\n",
        "-  Before evaluation, you should pick out $holdout$ items - the items our model will not see during the training. For this purpose, you need to sample $k$ elements from likes and favorites. You will need a holdout in the evaluation step. In this step, we predict the top $N$ recommended animes, and we expect that the holdout items will be within recommended items.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96299775",
      "metadata": {
        "id": "96299775"
      },
      "source": [
        "## EVALUATION\n",
        "\n",
        "\n",
        "1) In this task you need to compute metric $Recall@n$, $Recall$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "538dcf02",
      "metadata": {
        "id": "538dcf02"
      },
      "source": [
        "- In this task, you will solve the top $N$ recommendation problem. For this purpose, you need a more complex evaluation function.\n",
        "That's why we face so-called $Recall@n$. Namely,this metric takes as an input the list of personnel recommendations and holdout, and computes $Recall@n$ for each user separately. If our holdout animes are in the top $N$ recommendations for the user, the current recommendation is valid. To evaluate the models' ensemble, we need to average $Recall@n$ for all users where $animes_u$ - top  $N$ predicted animes for every user. \n",
        "\n",
        "\n",
        "$$\n",
        "Recall_u @n = \\frac{|anime_u \\cap holdout_u|}{|holdout_u|}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- To evaluate the whole model (model of models =)), you need to average personal recalls\n",
        "\n",
        "$$\n",
        "Recall = \\frac{1}{\\# users} \\sum_u Recall_u\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "822327ae",
      "metadata": {
        "id": "822327ae"
      },
      "outputs": [],
      "source": [
        "def cb_model_evaluate(recs, holdout, topn=10):\n",
        "    '''\n",
        "    Evaluate the recommendation system using the recall metric.\n",
        "\n",
        "    Parameters:\n",
        "    recs (numpy.ndarray): A 2D numpy array containing the recommended items for each user.\n",
        "                          The shape of the array is (num_users, num_items).\n",
        "    holdout (pandas.Series): A pandas Series containing the ground truth for each user.\n",
        "                             The index of the series corresponds to the user IDs and the values are lists\n",
        "                             of item IDs.\n",
        "    topn (int): The number of top recommendations to consider.\n",
        "\n",
        "    Returns:\n",
        "    float: The recall score of the recommendation system.\n",
        "    ''' \n",
        "    recall = []\n",
        "    recs = recs[:, :topn]\n",
        "    for idx, user_id in enumerate(holdout.index):\n",
        "        user_recs = recs[idx]\n",
        "        user_items = holdout.loc[user_id]\n",
        "        user_recall = len(set(user_recs).intersection(user_items))/len(user_items)\n",
        "        recall.append(user_recall)\n",
        "    return np.mean(recall)\n",
        "\n",
        "\n",
        "def sample_holdout(test_triplets, k=3):\n",
        "    '''\n",
        "    The function picks out holdout elements.\n",
        "    It chooses likes and favorites_anime for every user,   \n",
        "    shufflle obtained dataset,\n",
        "    groups it by user_id, \n",
        "    set the size of holdout\n",
        "    \n",
        "    '''\n",
        "    #   Complete the holdout function.\n",
        "    data = test_triplets.copy()\n",
        "    data[\"likes_and_fav\"] = data[\"likes\"] + data[\"favorites_anime\"]\n",
        "    grouped = data.drop([\"dislikes\", 'likes', 'favorites_anime'], axis=1)\n",
        "    holdout = {}\n",
        "    for user_id, likes_favs in grouped.itertuples(name=None):\n",
        "    #print(user_id) \n",
        "    #print(likes_favs)\n",
        "        np.random.shuffle(likes_favs)\n",
        "        holdout[user_id] = likes_favs[:k]\n",
        "    #print(holdout)\n",
        "\n",
        "    holdout = pd.Series(holdout)\n",
        "#####\n",
        "    # holdout = (test_triplets['likes'] + test_triplets['favorites_anime']).apply(np.unique)\n",
        "    # holdout.apply(random.shuffle)\n",
        "    # holdout = holdout.apply(lambda x: x[:k])\n",
        "\n",
        "    return holdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "15c5c30d",
      "metadata": {
        "id": "15c5c30d"
      },
      "outputs": [],
      "source": [
        "holdout = sample_holdout(test_triplets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "8813a6a7",
      "metadata": {
        "id": "8813a6a7"
      },
      "outputs": [],
      "source": [
        "cb_recall = cb_model_evaluate(cb_recs, holdout)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cb_recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emd5P-DZE1qw",
        "outputId": "3ba18e83-73cd-451e-c503-55415716dfd0"
      },
      "id": "Emd5P-DZE1qw",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01849148418491484"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfcf9723",
      "metadata": {
        "id": "dfcf9723"
      },
      "source": [
        "# Similarity Based models (8 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffbb9513",
      "metadata": {
        "id": "ffbb9513"
      },
      "source": [
        "The similarity-based approach is another attempt to personalize the content-based approach. We create so-called user profiles - the weighted sum of the TfIdf vectors of movies from user history to make user representations. Afterwards, we compute cosine similarities between user profile vectors and vectors of all the movies in the catalog.In the following part you need to improve similarity based approach presented on a lecture 2. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51673b7f",
      "metadata": {
        "id": "51673b7f"
      },
      "source": [
        "## USER PROFILE\n",
        "\n",
        "Let's have user $u_{i}$, who gave ratings $r_{i,j}$ to each anime $a_{j}$. (In our case $a_{j}$ is TfIdf representation of the current anime). So user profile vector will be the following:\n",
        "\n",
        "$$\n",
        "u_{i} = \\frac{\\sum_{j} (r_{i,j} \\times a_{j})}{\\sum_{j} r_{i,j}}\n",
        "$$\n",
        "\n",
        "In order to provide recommendations for the specific user we are going to compare an anime vector representation to user profile vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a184d2d",
      "metadata": {
        "id": "0a184d2d"
      },
      "source": [
        "\n",
        " \n",
        "**1)** Construct user profiles for users from test triplets. \n",
        "\n",
        "- In the first seminar, we used only users' positive reviews to create user profiles. Is this a good idea? Or we should use all the user's history? Comment on this question.\n",
        "\n",
        "\n",
        "**2)** Is cosine similarity the only similarity that can be chosen? Try different  [metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) and study how it affects the model performance.\n",
        "\n",
        "\n",
        "**3)** Construct the scoring function in the manner of CB-dased scoring function from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "42237c6d",
      "metadata": {
        "id": "42237c6d"
      },
      "outputs": [],
      "source": [
        "def user_profiles(tf_idf, test_pairs, reviews, anime):\n",
        "    aid_index_dict, index_aid_dict = re_index(anime, 'anime_id')\n",
        "    \n",
        "    user_profile = dict()\n",
        "    for i in test_pairs.index:\n",
        "        vec = 0\n",
        "        denom = 0\n",
        "        user_history = reviews[reviews.user_id == i][['anime_id', 'score']]\n",
        "        for ind in range(len(user_history)):\n",
        "\n",
        "          anime_id, score = user_history.iloc[ind]\n",
        "          if (anime_id not in test_pairs[test_pairs.index ==i].likes) and (anime_id not in test_pairs[test_pairs.index ==i].dislikes) and (anime_id not in test_pairs[test_pairs.index ==i].favorites_anime):\n",
        "            vec += tf_idf[aid_index_dict[anime_id]] *score\n",
        "            denom += score\n",
        "        user_profile[i] = vec/denom\n",
        "    return user_profile\n",
        "\n",
        "def re_index(dataframe, column):\n",
        "    '''\n",
        "    Naive reindexing of data via two dictionaries:\n",
        "    item-users and user-items mapping\n",
        "    '''\n",
        "    column_uniques = dataframe[column].unique()\n",
        "    indexes = np.arange(len(column_uniques))\n",
        "    item_index_dict = dict(zip(column_uniques, indexes))\n",
        "    index_item_dict = dict(zip( indexes, column_uniques))\n",
        "    return item_index_dict, index_item_dict    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "cb405084",
      "metadata": {
        "id": "cb405084"
      },
      "outputs": [],
      "source": [
        "def build_sim_model(config, trainset, trainset_description):\n",
        "    word_vectorizer = TfidfVectorizer(**config['tfidf'])\n",
        "    tfidf_matrix = word_vectorizer.fit_transform(trainset[trainset_description['item_features']])\n",
        "    users_profiles = user_profiles(tfidf_matrix, config['test_pairs'], config[\"reviews\"], config[\"anime\"])\n",
        "\n",
        "    return word_vectorizer,  users_profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "1e7260da",
      "metadata": {
        "id": "1e7260da"
      },
      "outputs": [],
      "source": [
        "\n",
        "sim_config = {\n",
        "    \"tfidf\": dict(\n",
        "        ngram_range = (1, 1),\n",
        "        min_df=5, max_df=0.9,\n",
        "        strip_accents='unicode',\n",
        "        stop_words = 'english',\n",
        "        analyzer = 'word',\n",
        "        use_idf=1,\n",
        "        smooth_idf=1,\n",
        "        sublinear_tf=1\n",
        "    ),\n",
        "    \"reviews\" : reviews_train, #reviews_cut, #reviews_clean,\n",
        "    \"anime\" : anime_cut, #anime_train,  reviews_train\n",
        "    'test_pairs' : test_triplets #test_pairs\n",
        "}\n",
        "\n",
        "\n",
        "anime_description = {\n",
        "    'feedback' : \"rating\",\n",
        "    \"items\": \"anime_id\",\n",
        "    \"item_features\": \"synopsis\",}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "028038d7",
      "metadata": {
        "id": "028038d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b25ab43-789a-4413-8a62-7575fd1af83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/_param_validation.py:541: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "params = build_sim_model(sim_config, anime_cut, anime_description)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "EvXMQtVYdxMo"
      },
      "id": "EvXMQtVYdxMo",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_recommendations(params, training, testset, data_description, topn=10):\n",
        "    \"\"\"\n",
        "    Uses an ensemble of individual content-based models to generate recommendations for each test user.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : dict\n",
        "        A dictionary containing the regression model and word vectorizer for each user.\n",
        "    training : pandas.DataFrame\n",
        "        DataFrame containing the user ID, item ID, and rating for each item in the training dataset.\n",
        "    testset : pandas.DataFrame\n",
        "        DataFrame containing the user ID, liked items, disliked items, and favorite items for each test user.\n",
        "    data_description : dict\n",
        "        A dictionary containing metadata information for the dataset.\n",
        "        - feature_map : pandas.DataFrame\n",
        "            DataFrame containing the item ID and feature representation for each item.\n",
        "        - users : str\n",
        "            The name of the user ID column.\n",
        "        - items : str\n",
        "            The name of the anime ID column.\n",
        "        - train_items : numpy.ndarray\n",
        "            Array containing the unique item IDs from the training dataset.\n",
        "    topn : int, optional\n",
        "        The number of recommendations to generate per user. Default is 10.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "        Array containing the top n recommended anime IDs for each test user\n",
        "        with preserved ordering of rows corresponding to the order of users in testset.\n",
        "\n",
        "    TL;DR\n",
        "    -----\n",
        "    This function generates item recommendations for test users using a content-based model.\n",
        "    For each user in the testset, the function selects items not in the user's history and \n",
        "    combines them with the user's likes, dislikes, and favorites.\n",
        "    The function then applies the user's regression model to the feature representation of \n",
        "    these items and generates a score for each of them. The top-n scoring items are recommended to the user.\n",
        "\n",
        "    \"\"\"\n",
        "    word_vectorizer,  users_profiles = params\n",
        "\n",
        "    training = sim_config['reviews']\n",
        "    anime = sim_config['anime']\n",
        "    test_triplets = sim_config['test_pairs']\n",
        "\n",
        "    anime_index_dict, _ = re_index(anime, 'anime_id')\n",
        "\n",
        "    user_history = training.groupby('user_id')['anime_id'].apply(list)\n",
        "    all_items = training['anime_id'].unique()\n",
        "    tfidf_matrix = word_vectorizer.transform(anime['synopsis'])\n",
        "\n",
        "    recs = []\n",
        "    for user_id, dislikes, likes, favs in test_triplets.itertuples(name = None):\n",
        "      # print(\"user_id\", user_id)\n",
        "      # print('dislikes', dislikes)\n",
        "      # print('likes', likes)\n",
        "      # print('favs', favs)\n",
        "      items_not_from_history = np.setdiff1d(all_items, user_history[user_id]).tolist()\n",
        "      # print('items_not_from_history', items_not_from_history)\n",
        "      scoring_items = np.array(items_not_from_history + dislikes + likes + favs)\n",
        "      # print('scoring_items', scoring_items)\n",
        "      anime_index = [anime_index_dict[anime_id] for anime_id in scoring_items]\n",
        "      # print('anime_index', anime_index)\n",
        "      # print('tfidf_matrix[anime_index]', tfidf_matrix[anime_index])\n",
        "      # print('user_profiles', users_profiles)\n",
        "      # print('user_profiles[user_id]', users_profiles[user_id])\n",
        "      similarity_scores = cosine_similarity(tfidf_matrix[anime_index], users_profiles[user_id]).squeeze()\n",
        "\n",
        "      recs.append(scoring_items[similarity_scores.argsort()[-topn:]])\n",
        "\n",
        "    return np.array(recs)"
      ],
      "metadata": {
        "id": "RE40_M0mjiX_"
      },
      "id": "RE40_M0mjiX_",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recs = model_recommendations(params, reviews_train, test_triplets, anime_description)"
      ],
      "metadata": {
        "id": "Rc-H0UxrjibS"
      },
      "id": "Rc-H0UxrjibS",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCphNlMpjied",
        "outputId": "5467a3ba-dd94-40a2-e41b-c090d3b740f4"
      },
      "id": "HCphNlMpjied",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[34541,   488, 14289, ..., 11979, 31564, 11977],\n",
              "       [31564, 30355,   878, ..., 16762, 16009,  5342],\n",
              "       [ 5365, 31793, 32526, ..., 33926, 33926, 38993],\n",
              "       ...,\n",
              "       [14199,   389, 37281, ...,   977, 16782, 32491],\n",
              "       [38276, 34284,  7058, ..., 32979,   358,  1169],\n",
              "       [25173, 23755, 11703, ...,    58,  3583,  6773]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "zlhFErdX6WQd"
      },
      "outputs": [],
      "source": [
        "holdout = sample_holdout(test_triplets)"
      ],
      "id": "zlhFErdX6WQd"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "rZZWFU0e6WQd"
      },
      "outputs": [],
      "source": [
        "recall = cb_model_evaluate(recs, holdout)"
      ],
      "id": "rZZWFU0e6WQd"
    },
    {
      "cell_type": "code",
      "source": [
        "recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5958f9bc-27ef-4e50-ccaa-ad1640026259",
        "id": "FnH6-NI46WQd"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02871046228710462"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "id": "FnH6-NI46WQd"
    },
    {
      "cell_type": "markdown",
      "id": "45c2b621",
      "metadata": {
        "id": "45c2b621",
        "outputId": "776187c3-3b9d-4e51-d85a-63daf1cd43c8"
      },
      "source": [
        "# MODELS COMPARISON (2 PTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de76abe",
      "metadata": {
        "id": "6de76abe"
      },
      "source": [
        "**1)** Compare the discussed model. Which of them works better and why? Comment on your results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content-based models with personalization: Recall 0.018\n",
        "\n",
        "Similarity Based models: Recall 0.029\n",
        "Second model works better, because it takes into account a greater number of model parameters"
      ],
      "metadata": {
        "id": "erPcn_PyOGo7"
      },
      "id": "erPcn_PyOGo7"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_KmGtHVTO7u"
      },
      "id": "K_KmGtHVTO7u",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "64cd544b7330e8e73b8689d110cc075e8c836a404445c2b82c04f3ea96ea86ff"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}